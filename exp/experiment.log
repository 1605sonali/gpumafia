11.12.2012

1)
Had a problem with too sensitive MAFIA algorithm detecting all windows in
uniform dimensions as dense. Clearly, these 1D dense units do not propagate in
higher dimensions, as max * width density test is used only for 1D CDUs. The
solution is to desensitize the algorithm with:

- smaller number of bins (-n100)
- larger number of windows (-M100)
- larger alpha threshold (-a2.2)

either option desensitizes the algorithm sufficiently for it to start producing
only the intended central cluster, as generated by the data generator. Other
insignificant 1D clusters may still be produced, but they no longer cover the
entire data set.

2) Generated a number of test data sets. All of them contain 20_000
20-dimensional points. The domain for each dimension's coordinate is 0..100, and
the domain for the cluster in each dimension is 42..68. The point is generated
inside the cluster with probability 0.9, and in the entire domain (which does
not exclude the cluster, of course), with probability 0.1. Each dataset contains
a single cluster, and the dimensionality of the cluster varies from 2 to 19. For
files with dimension up to 11, the entire problem is IO-dominated. For higher
dimensions, on the other hand, the exponent caused by the large number of
subspace clusters generated dominates. The time grows exponentially: 0.68 s
for k=11, 1.33 s for k=12, 3.1 s for k=13, 10.5 s for k=14, 41.8 s for k=15 and
174.5 s for k=16. Now will do some profiling to determine what part of the
algorithm dominates: the one connected to DUs only, or the one connected to
counting points in DUs.

3) Performed the experiment (experiment-01) on the data mentioned in p. 2). It
is clear that after I/O stops dominating, point counting (find_dense_cdus())
dominates, util around k=13. Starting from that, the fraction of time spent on
deduplication (dedup_cdus()) only increases. Deduplication the same time as
point counting for k=14, and clearly dominates with k>=15. So, for higher
dimensions, doing deduplication in N * lg N time instead of N * N can clearly
improve performance.

Further improvements shall concentrate on:
- deduplication in N * lg N time
- using bitmaps for counting points (should improve the overall time, not
asymptotical time)

12.12.2012

1) Implemented both deduplication in N * lg N time and point counting using
bitmaps. This reduced computing time drastically: for k=14, everything runs in
around 0.7 s, and for k=16, it takes just 12 s. It looks like using bitmaps
improved the speed of point counting about 6 times, and the set-based
deduplication had cut the time spent in deduplication phase
significantly. Profiling will show more specific results achieved

2) Profiling (experiment-02) has shown that for 20_000 points and cluster
dimension k>10, point counting with bitmaps takes absolutely insignificant
amount of time compared to CDU generation+deduplication or finding unassimilated
DUs. In fact, using bitmaps reduced the time spent in point counting by more than two
orders of magnitude. However, experiment-02 exercises only a single important
case, the one with the large number of CDUs. An experiment with more points need
to be performed, to be able to estimate the effect the many points have on the
data processing speed. 1-10M points with 10 dimensions and the dimensionality of
the cluster varying from 2 to 10 should do that

3) Testing with a larger number of points (1M, 10M) in 10 dims with up to 10-dim
cluster in experiments 03 and 04 demonstrated that most time is spent in
computing the initial histogram. This is a surprise, as we originally expected
most of the time to be spent in the inner loop. It looks like all the time is
spent that way due to poor memory layout: points are stored in point-first order
instead of dimension-first order. What is now required is committing the data,
changing the point storage order, and trying again with the new order.

4) Changing the data layout seems to improve the computation time, at least at
first sight. More detailed profiling required, of course, to provide precise
estimates. Experiments (experiment-05) indicate that transposition has a
positive effect on building the histogram, but worsens the time by around 5%
when direct (non-bitmap) point counting is used. 

13.12.2012

1) It turned out that gprof profiling data were grossly inaccurate. In
particular, it underreported I/O significantly: I/O really takes nearly 90% of
the entire computation time (in case of bitmaps). However, the overall situation
remains the same: using bitmaps is an order of magnitude faster (though there's
still an upfront cost of building them all). The time spent in finding dense
CDUs is underreported; it is higher, and plays a significant role once
k>=9. Until then, the histogram computation is the major part, and it's the
prime candidate for offloading to the device (of course, there's also
construction of bitmaps). Now we'll do another experiment (experiment-06),
without profiling.

14.12.2012

1) Porting to CUDA, now computing histograms on device. Using the device gives
some acceleration, even on 100K points. It should be better for 10M points, for
which we'll now do experiments.

17.12.2012

1) The application runs on CUDA. Now trying to use pinned host memory instead of
usual malloc() to allocate bulk point data.

2) Using pinned host memory for bulk allocation reduced histogram computation
time (which includes copying) from 0.42s to 0.32s for 10M 10d points. Now will
use proper atomics in histogram computation. 

3) Using atomics with shared memory reduced the overhead greatly. Histogram
computation now takes about 0.17-0.18 s, of which roughly 0.16 s is moving data
from host (pinned memory) to GPU (at the rate of 6 GB/s). It makes no sense to
optimize histogram processing further. Neither is doing overlap with streams
useful, as the bulk of the time is taken by communication. What is required is
to do a commit, then do host vs. GPU measurements, and send the report.

19.12.2012

1) Moved point counting to GPUs. This gave 5-50x improvement to point counting
phase (depending on the number of points, more points meanes more improvement),
and overall 5-10x improvement to the time spent in computational phase. 

15.01.2013

1) Now generating multiple clusters, mainly box clusters. Detection quality of
MAFIA algorithm is not so good as with a single cluster. The particularly
complicated cases are those in which clusters intersect in some dimensions. In
this case, detection goes wroing even in the simplest 2D case with 5 clusters:
MAFIA reports only single-dimensional clusters. With 3-4 clusters in 2D case,
the situation can be fixed with more bins and windows (-n400 -M100), but this
does not always help. For higher dimensions, the detection rate in case of
multiple clusters (4 2d clusters) is even lower: everything is detected in 1
dimension only, and is for some reason not merged into 2d. 

16.01.2013

1) Now trying to parallelize computationally intensive parts of the algorithm on
CPU using OpenMP. For point counting, tried applying omp for to the loop on CDUs
and the loop on bitmap words. So far, it seems that applying it to both
directives the most stable performace, so leaving it this way. Parallelizing
only the inner loop (on words) is slower, while parallelizing only the outer
loop (on CDUs) leads to less stable performance.

2) Parallelized building bitmaps with OpenMP. This required modification of bitmap
computation. omp for was applied to both loops (on windows and on words), to
achieve the most flexibility.

3) Parallelized histogram and extent computation with OpenMP. Only the outer
loop was parallelized for computation, as OpenMP for C doesn't support min/max
reduction. And this is bad, because it limits acceleration that can be achieved
there with high number of cores or low data dimensionality. For histogram
construction, both outer and inner loops were parallelized. For the inner loop,
histograms were privatized for each thread, and then accumulated.

4) The preliminary experiment involved 1000,000 20-d points, with a single 10-d
cluster. The experiment was done on my notebook, which is a 2011 11.6"
MacbookAir with a 1.6 GHz Sandy Bridge CPU. The CPU has 2 2-way-hyperthreaded
cores, and the number of threads used by OpenMP is 4. On average, around 2x
acceleration was achieved in all parts of the MAFIA algorithm where OpenMP
directives were used. Now will add --seq option for disabling parallelism, and
do an experiment on a cluster.

17.01.2013

1) Spent some time in understanding as to why the parallel program runs on the
head node, but crashes on the compute node of JUDGE system. As it turned out,
I've generated 10,000,000 20-d points instead of 10-d points, and ran afoul of
virtual memory limits on compute nodes. With 10-d points, everything works fine,
results match, and the scene is ready for two more experiments: CPU performance
for different number of threads, and seq. CPU vs. parallel CPU vs GPU
performance for different cluster sizes.

2) Starting doing OpenMP experiments on JUDGE. I currently decided to simply
trust (if I ever can do that) to OpenMP scheduling. This experiment
(experiment-08) deals with a single file, which contains 10,000,000 10-d points,
with a single cluster of dimensionality 10. The number of OpenMP threads varied
from 1 (full sequential) to 24. As the CPU is two-way hyperthreaded,
acceleartion reaches a plateau at 12 cores. Histogram gets about 7x times
acceleration, bitmap construction about 6x, and point counting 9x. This is
definitely better than the sequential case, but still worse than using the GPU. 

In future, it is possible to experiment with different OpenMP thread affinity
settings. But for now, numbers obtained using OpenMP indicate that default
scheduling is reasonable.

3) Compared sequential CPU vs. parallel CPU vs. GPU implementations. Performance
testing was again performed on 10,000,000 10-d points, with cluster
dimensionality varying from 2 to 10. Parallelization on a 12-core dual-socket
CPU removes much of the benefit of using the GPU. When the point data transfer
time is taken into account, GPU implementation is only about 1.5-2x faster than
the parallel CPU implementation. While the main phases, such as histogram
building (excluding data transfer) and point counting still receive significant
acceleration, respectively about 30x and 5x, this is eaten away by the need to
transfer data to GPU, and also by steps not accelerated on the GPU, such as
cluster expansion. 

Transferring bitmap back to host side takes time as well. However, after final
cluster construction is moved to GPU, something can be shaved off GPU time, to
provide more acceleration. Kepler comparison is also waiting. Performance per
watt comparison may be also beneficial, especially for the Kepler case. However,
it must be admitted that after CPU parallelization, GPU performance domination
is undermined significantly.

21.01.2013

1) It looks like we also need an intermediate parallel experiment, with 4 CPU
cores (par4 in experiment-09), to demonstrate GPU acceleration in a somewhat
more realistic setup. In this case, acceleration is around 2.5-3 times, which is
not as great as for a single-core case (5-15 times). 

28.01.2013

1) Now doing generation using set algorithm. That is, for every (a-1)-DU, we generate
all possible (a-2)-subsequences, then for every subsequence, we generate a list
of DUs containing this subsequence. Finally, only those DUs which contain the
common subsequence are checked for merging. It looks like in the case of small k
(cluster dimensionality), the overhead introduced by more complex operations is
small, if any. For larger k, however, the gain is pretty noticeable. However, as
there's still exponent there, with larger k, the algorithm doesn't run as fast
as we'd like. Now we need to implement set-based algorithm for unjoined check as
well, and then proceed to experiments. In fact, for k >= 17, time for unjoined
check still dominates.

The existence of the range where the real costs of naive and set algorithms are
similar raises the question of whether it is possible to port the naive
algorithm to GPU to gain more acceleration. However, there are several points
against it:

- due to the difference between exponents, such a range is very short. In fact,
we believe that for the current hardware, the difference between approaches for
k >= 18 is so large that even the most powerful GPUs won't be able to bridge it.
- generation and deduplication of CDUs on GPU requires lots of very irregular
memory allocation, for which current GPUs are ill-suited. This further decreases
obtainable acceleration.
- unjoined check, or a part of it, can be accelerated on GPU. However, it has
the same order of magnitude in costs as the generation, and so accelerating it
will not increase performance much.

All this said, there is still an open question whether it would be possible to
parallelize gen, dedup and unjoin check on CPU. While the trivial algorithm is
parallelizable, the optimized algorithm would require a multi-threaded
hashtable. And while the implementation of one is available, multi-threading
will certainly introduce its overhead. For the time being, so, we consider only
algorithmic optimization of the gen/dedup/unjoin kernels, without considering
its parallelization.

2) Implemented set algorithm for assimilation check. Now it's time to see
whether set unjoined check gives worse performance for small cluster
dimensionalities. It looks like there is a small performance drop up to k=13,
but after that, set-based unjoined check is better. Now its time for more
serious experiments, to estimate performance differences and compare various
hardware architectures.

3) Doing an experiment (experiment-10) comparing single-core timing with all
algorithmic optimizations implemented so far, and with all optimizations
disabled (--no-set-dedup --no-set-gen --no-bitmaps). 

29.01.2013

1) Fixed a bug in the generator which, when multiple clusters were being
generated, caused all clusters to be generated on 0..1 (instead of, say, 32..42)
in some dimensions. This improved cluster detection rate, but when clusters
intersect in some of the dimensions, they are still not reliably detected. Now
trying out the dimensionality of clusters which can be detected reliably, for
the fixed number of clusters. The tests are conducted for d=20 and standard
parameter values.

m								 k
2								 8 (mostly reliably, though 7 is better)
3								 6 (7 is already around 50/50)
4								 3 (4 is much less reliable)

It looks like for test purposes, non-intersecting clusters, maybe aligned to
integer boundaries, should be generated. So, going to implement generating
non-intersecting clusters in clugen now. 

2) Implemented generating non-intersecting clusters, i.e. clusters which do not
share their spans in any of the dimensions. It turns out that such clusters are
very useful for providing just right-tuned input for MAFIA algorithm. MAFIA
easily detects up to 5 non-intersecting clusters without parameter tuning. Some
low-dimensional garbage is detected as well, but this is the typical behavior of
MAFIA. For more than 5 non-intersecting clusters or lots of noise, alpha must be
lowered. In fact, for many practical cases, a value of alpha < 1 may be useful,
as found by Jan. But now we will do timing experiments with clusters comparing
performance of different versions of the algorithm, first on JUDGE, and then on
Kepler. 

30.01.2013

1) Conducted the experiment with multiple clusters. The dataset series consisted of
10,000,000 20-d points, with three clusters in each (not intersecting in any
dimension, so to be reliably detected), with cluster dimensionality varying from
3 to 16 across the series. We dropped a set with a 17-dimensional cluster after
it turned out that it took too much time to run it. With increasing cluster
dimensionality, GPU fairs better: for k=16, it gives 4x acceleration over
12-core CPU, and around 40x over a single core. Now will do the experiment with
Kepler, and then build the plots of the results.

