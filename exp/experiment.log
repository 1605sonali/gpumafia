11.12.2012

1)
Had a problem with too sensitive MAFIA algorithm detecting all windows in
uniform dimensions as dense. Clearly, these 1D dense units do not propagate in
higher dimensions, as max * width density test is used only for 1D CDUs. The
solution is to desensitize the algorithm with:

- smaller number of bins (-n100)
- larger number of windows (-M100)
- larger alpha threshold (-a2.2)

either option desensitizes the algorithm sufficiently for it to start producing
only the intended central cluster, as generated by the data generator. Other
insignificant 1D clusters may still be produced, but they no longer cover the
entire data set.

2) Generated a number of test data sets. All of them contain 20_000
20-dimensional points. The domain for each dimension's coordinate is 0..100, and
the domain for the cluster in each dimension is 42..68. The point is generated
inside the cluster with probability 0.9, and in the entire domain (which does
not exclude the cluster, of course), with probability 0.1. Each dataset contains
a single cluster, and the dimensionality of the cluster varies from 2 to 19. For
files with dimension up to 11, the entire problem is IO-dominated. For higher
dimensions, on the other hand, the exponent caused by the large number of
subspace clusters generated dominates. The time grows exponentially: 0.68 s
for k=11, 1.33 s for k=12, 3.1 s for k=13, 10.5 s for k=14, 41.8 s for k=15 and
174.5 s for k=16. Now will do some profiling to determine what part of the
algorithm dominates: the one connected to DUs only, or the one connected to
counting points in DUs.

3) Performed the experiment (experiment-01) on the data mentioned in p. 2). It
is clear that after I/O stops dominating, point counting (find_dense_cdus())
dominates, util around k=13. Starting from that, the fraction of time spent on
deduplication (dedup_cdus()) only increases. Deduplication the same time as
point counting for k=14, and clearly dominates with k>=15. So, for higher
dimensions, doing deduplication in N * lg N time instead of N * N can clearly
improve performance.

Further improvements shall concentrate on:
- deduplication in N * lg N time
- using bitmaps for counting points (should improve the overall time, not
asymptotical time)

12.12.2012

1) Implemented both deduplication in N * lg N time and point counting using
bitmaps. This reduced computing time drastically: for k=14, everything runs in
around 0.7 s, and for k=16, it takes just 12 s. It looks like using bitmaps
improved the speed of point counting about 6 times, and the set-based
deduplication had cut the time spent in deduplication phase
significantly. Profiling will show more specific results achieved

2) Profiling (experiment-02) has shown that for 20_000 points and cluster
dimension k>10, point counting with bitmaps takes absolutely insignificant
amount of time compared to CDU generation+deduplication or finding unassimilated
DUs. In fact, using bitmaps reduced the time spent in point counting by more than two
orders of magnitude. However, experiment-02 exercises only a single important
case, the one with the large number of CDUs. An experiment with more points need
to be performed, to be able to estimate the effect the many points have on the
data processing speed. 1-10M points with 10 dimensions and the dimensionality of
the cluster varying from 2 to 10 should do that

3) Testing with a larger number of points (1M, 10M) in 10 dims with up to 10-dim
cluster in experiments 03 and 04 demonstrated that most time is spent in
computing the initial histogram. This is a surprise, as we originally expected
most of the time to be spent in the inner loop. It looks like all the time is
spent that way due to poor memory layout: points are stored in point-first order
instead of dimension-first order. What is now required is committing the data,
changing the point storage order, and trying again with the new order.

4) Changing the data layout seems to improve the computation time, at least at
first sight. More detailed profiling required, of course, to provide precise
estimates. Experiments (experiment-05) indicate that transposition has a
positive effect on building the histogram, but worsens the time by around 5%
when direct (non-bitmap) point counting is used. 

13.12.2012

1) It turned out that gprof profiling data were grossly inaccurate. In
particular, it underreported I/O significantly: I/O really takes nearly 90% of
the entire computation time (in case of bitmaps). However, the overall situation
remains the same: using bitmaps is an order of magnitude faster (though there's
still an upfront cost of building them all). The time spent in finding dense
CDUs is underreported; it is higher, and plays a significant role once
k>=9. Until then, the histogram computation is the major part, and it's the
prime candidate for offloading to the device (of course, there's also
construction of bitmaps). Now we'll do another experiment (experiment-06),
without profiling.

